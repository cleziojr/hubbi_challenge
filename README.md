# Desafio T√©cnico - Hubbi

Este projeto implementa uma **pipeline automatizada de ETL** que extrai dados de produtos de um site de testes, processa e armazena em um **banco PostgreSQL**.  

A solu√ß√£o √© **100% containerizada**, utilizando **Docker Compose** para orquestrar:  

- A aplica√ß√£o Python (`app`)  
- O navegador para web scraping (`selenium/firefox`)  
- O banco de dados (`PostgreSQL`)  

---

## üõ†Ô∏è Pr√©-requisitos

Para rodar o projeto, voc√™ precisa ter instalado:  

1. **Git** ‚Äì para clonar o reposit√≥rio.  
2. **Docker e Docker Compose v2** ‚Äì essencial para executar toda a pipeline.  
3. **Python 3.8+** (opcional) ‚Äì apenas se desejar consultar o notebook localmente.  

---

### Guia de instala√ß√£o dos pr√©-requisitos

**Windows e macOS:**  

- Instale o Docker Desktop. O Docker Compose v2 j√° vem inclu√≠do.  

**Linux (Ubuntu/Debian):**  

```bash
sudo apt-get update
sudo apt-get install docker.io docker-compose-plugin
````

> ‚ö†Ô∏è Recomendado: siga os passos p√≥s-instala√ß√£o do Docker para usar sem privil√©gios de root.

```bash
docker --version
docker compose version
```

---

## üíª Observa√ß√µes sobre Docker em diferentes sistemas operacionais

### Windows e macOS

* O **Docker Desktop** j√° inclui Docker Engine e Docker Compose v2.
* Comandos s√£o exatamente os mesmos que os listados abaixo.

### Linux (Ubuntu/Debian)

* Pode ser necess√°rio usar `sudo` para os comandos Docker:

```bash
sudo docker compose up -d --build
sudo docker compose logs -f app
sudo docker compose down
```

* Para n√£o precisar usar `sudo` sempre, adicione seu usu√°rio ao grupo docker:

```bash
sudo usermod -aG docker $USER
```

Depois fa√ßa logout/login para aplicar.

### Observa√ß√µes gerais

* Todos os comandos devem ser executados **dentro do diret√≥rio do projeto**, onde est√° o `docker-compose.yml`.
* Use sempre `docker compose` (v2) e n√£o o antigo `docker-compose`.
* Em PowerShell/Windows, caminhos relativos podem usar `.\` em vez de `./`.

---

## Executando a pipeline

### 1. Clone o reposit√≥rio

```bash
git clone https://github.com/cleziojr/hubbi_challenge.git
cd hubbi_challenge
```

### 2. Suba os containers

```bash
docker compose up -d --build
```

O comando cria as imagens e inicia os servi√ßos `app`, `db` e `selenium-firefox`. O script `main.py` ser√° executado automaticamente dentro do container `app`.

### 3. Acompanhe os logs de execu√ß√£o

```bash
docker compose logs -f app
```

> Pressione `Ctrl+C` para sair dos logs (isso **n√£o serve para os containers**).

### 4. Consulte a an√°lise explorat√≥ria (EDA)

O projeto inclui um **notebook `eda_hubbi.ipynb`** com an√°lises explorat√≥rias e gr√°ficos sobre os dados coletados.
Ele pode ser **consultado diretamente como arquivo `.ipynb`**, sem necessidade de execu√ß√£o, permitindo revisar gr√°ficos, tabelas e insights obtidos.

### 5. Desligue a pipeline

```bash
docker compose down
docker compose down -v
```

---

## üí° Decis√µes de design e arquitetura

### Estrat√©gia de ETL

* **Carga de dados:** a carga no banco de dados √© efetuada a cada p√°gina de produtos iterada. Esta decis√£o foi um *trade-off* consciente para alcan√ßar um equil√≠brio entre performance e persist√™ncia:

  * √â mais seguro do que efetuar a carga apenas ao final da execu√ß√£o (o que elevaria a performance, mas diminuiria a garantia de persist√™ncia em caso de falha).

  * √â muito mais perform√°tico do que efetuar a carga a cada produto (o que diminuiria drasticamente a performance, mas garantiria ao m√°ximo a persist√™ncia).

* **Integridade dos dados:** antes de inserir os dados de um produto no banco, o algoritmo sempre verifica se aquele produto j√° foi inserido anteriormente. Desta forma, garante-se a integridade e apenas os dados novos sobem para o banco, permitindo que a pipeline seja executada repetidas vezes sem gerar duplicidade.

* **Ferramenta de extra√ß√£o:** optei pelo selenium para a extra√ß√£o dos dados. Embora eu j√° tenha criado pipelines utilizando requests e beautifulsoup para parsear e extrair o conte√∫do, o selenium foi escolhido neste projeto por eu achar mais f√°cil de lidar, o que permitiu um desenvolvimento mais √°gil.

### Infraestrutura

* **Containeriza√ß√£o:** Docker garante ambiente consistente e port√°vel.
* **Orquestra√ß√£o:** `docker-compose` gerencia `app`, `db` e `selenium-firefox`.

### Seguran√ßa e credenciais

* As credenciais do banco de dados (usu√°rio, senha, host) est√£o armazenadas em vari√°veis de ambiente, lidas a partir de um arquivo .env. Esta √© uma pr√°tica de seguran√ßa fundamental para proteger dados sens√≠veis.

* **Disclaimer para avalia√ß√£o:** tenho ci√™ncia de que, em um ambiente de produ√ß√£o, o arquivo .env **nunca** √© "commitado" no reposit√≥rio. Para facilitar a execu√ß√£o e corre√ß√£o deste desafio, o arquivo .env foi excepcionalmente inclu√≠do.

---

## An√°lise explorat√≥ria dos dados (EDA)

* **Foco na ruptura de estoque:** durante a an√°lise, foi observado (conforme detalhado no notebook eda_hubbi.py) uma grande similaridade no comportamento estat√≠stico de diversas vari√°veis (como pre√ßo, peso e dimens√µes).

* Dada a baixa varia√ß√£o estat√≠stica em atributos como pre√ßo e dimens√µes, a an√°lise foi direcionada ao insight de maior relev√¢ncia para a empresa: a falta de estoque (ruptura). Aplicando o princ√≠po de pareto (80/20), pude identificar que o problema est√° altamente concentrado. Isso permite que, ao inv√©s de uma solu√ß√£o gen√©rica, seja poss√≠vel priorizar categorias e marcas espec√≠ficas que trar√£o o maior impacto na redu√ß√£o de perdas por ruptura.

